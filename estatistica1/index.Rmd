---
title: "Análise Estatística Multivariada"
author: "João Ricardo F. de Lima"
date: "`r format(Sys.time(), '%d de %B de %Y.')`"
output: 
    html_document:
        theme: flatly
        number_sections: yes
        highlight: textmate
#        includes: 
#          in_header: "header.html"
        toc: yes
        toc_float:
          collapsed: yes
          smooth_scroll: yes 
---

<br>

# Conceito, Objetivos e Dados

<br>

Consiste em um conjunto de métodos estatísticos usados para simplificar a interpretação de grandes conjuntos de dados. A aplicação pode ocorrer em diversas áreas do conhecimento.

Segundo Mingoti (2005, p. 21), *"embora historicamente o uso de métodos multivariados esteja relacionado com trabalhos na Psicologia, Ciências Sociais e Biológicas, mais recentemente eles têm sido aplicados em um grande universo de área diferentes, como: Educação, Geologia, Química, Física, Engenharia, Ergonomia, etc."*.

Basicamente os **objetivos** são:

a) Redução ou simplificação da base de dados sem sacrificar informação importante;
 
b) Ordenação e agrupamento (classificação) - criar grupos de objetos (ou variáveis) semelhantes com base em várias características. Por isso é multivariada;

c) Investigação da estrutura de dependência entre as variáveis. Podemos ter um conjunto de dados relacionados com tecnologias utilizadas nas fazendas e outro conjunto de dados relacionados com característica do dono da propriedade. A dependência entre estes grupos de variáveis pode indicar porque algumas propriedades usam mais tecnologias do que outras;

d) Predição - realizar previsões;
 
e) Construção de hipóteses e testes.

As técnicas de Análise multivariada são muito utilizadas para a construção de índices. Um índice sintetiza em uma única variável a informação de todas as variáveis que foram medidas sobre o fenômeno. Como exemplo, pode-se citar Índice de Desemprego, Índice de Qualidade de Vida, Índice de Qualidade Ambiental, Índice de Desenvolvimento Humano (IDH), entre outros. Nestes casos, a *Análise de Componentes Principais* ou *Análise Fatorial* são técnicas que podem ser utilizadas (LIMA, 2015). 

Outra aplicação interessante é quando se procura dividir um conjunto de objetos em grupos que sejam os mais homogêneos possíveis dentro do grupo e heterogêneos entre si. Uma empresa pode buscar estratificar seus clientes, dividindo-os em grupos e traçando objetivos para cada grupo separadamente. Pode-se separar os municípios de determinado estado em grupos homogêneos com o intuito de delinear programas adequados para os diferentes grupos. Nestes casos, a *Análise de Cluster* é a técnica adequada (LIMA, 2015). 

Outra situação é a enfrentada pelas seguradoras para realização de seguros de veículos. Cada cliente se encaixa em determinado perfil que pode ter maior ou menor risco de sinistro. Neste caso, pode-se aplicar *Análise Discriminante* buscando identificar a qual grupo de risco de sinistro um determinado elemento tem mais chance de pertencer (LIMA, 2015).

Os **dados para Análise multivariada** em uma amostra de seção cruzada são geralmente compostos de variáveis (renda, idade, anos de escolaridade, etc.) e observações (pessoa, família, escola, município, estado, país, etc.). Dispostos em uma tabela os dados formam uma matriz em que cada coluna se refere a uma variável e cada linha a uma observação. As **observações** são as unidades de Análise ou unidades amostrais e as **variáveis** são as características medidas nas observações. Assim, os dados usados são tipicamente formados por *n* observações em *p* variáveis.

```  {r estat1, warning=FALSE, message=FALSE}
#Verificando o diretorio que o R esta direcionado
getwd()

#Direcionado o R para o Diretorio a ser trabalhado
setwd('/Users/jricardofl/Dropbox/tempecon/dados_censoagro')

#Pacote
library(readxl)

#Entrada dos dados
dados <- read_excel("artigo.xlsx")

# Nomes das variáveis utilizadas
#
#x1	Porcentagem dos estabelecimentos que usam força animal
#x2	Porcentagem dos estabelecimentos que usam força mecânica
#x3	Porcentagem da área com pastagens que é plantada
#x5	area trabalhada como porcentagem da área aproveitável
#x6	Area com lavouras permanentes e temporárias como proporção da área aproveitável
#x7	Numero de tratores por equivalente homem
#x8	Numero de tratores por área explorada
#x9	Numero de arados por área explorada
#x10	Numero de colheitadeiras por área explorada
#x11	Valor total dos combustíveis consumidos por área explorada
#x12	Quantidade de energia eletrica consumida por área explorada
#x13	Quantidade de energia eletrica consumida po equivalente homem
#x14	Valor total dos bens por área explorada
#x15	Valor total dos bens por equivalente homem
#x16	valor total dos investimentos por area explorada
#x17	Valor total dos investimentos por equivalente homem
#x18	Valor total dos financiamentos em 2006 por área explorada
#x19	Valor total dos financiamentos em 2006 por equivalente homem.
#x20	Valor total da produção em 2006, por área explorada
#x21	Valor total da produção em 2006, por equivalente-homem
#x22	Valor total das despesas em 2006, por área explorada
#x23	Valor total das despesas em 2006, por equivalente homem
#x24	Despesas com adubos, corretivos, sementes e mudas, agrotóxicos, medicamenteos para animais, sal e rações por área explorada
#x25	Despesas com adubos, corretivos, sementes e mudas, agrotóxicos, medicamenteos para animais, sal e rações por equivalente homem
#x26	Assistência Técnica
#x27	uso de agrotóxico para controle de pragas e doenças
#x28	uso de controle alternativo de pragas e doenças
#x29	irrigação

#Visualizaçao dos dados
head(dados, 15)

summary(dados)
```

<br>

# Técnicas da Análise Multivariada

<br>

Pode-se considerar que a estatística multivariada se divide em dois grupos: o primeiro consiste nas técnicas de simplificação da estrutura de variabilidade dos dados. Principalmente, fazem parte deste grupo a **Análise de Componentes Principais, Análise Fatorial, Correlações Canônicas, Cluster e Discriminante**.
	
O segundo grupo concentra os métodos de estimação de parâmetros, como na análise de **Regressão Simples** e **Múltipla**. 

Para um estudo aprofundado das técnicas de *Análise Multivariada*, é importante revisar conceitos de Estatística Básica: média, variância, desvio padrão, covariância, correlação seriam as mais relevantes.

E também é importante revisar conceitos de álgebra matricial: vetores, matrizes, combinações lineares, dependência linear, raízes e vetores característicos e decomposição espectral, basicamente. 

<br>

## Análise de Componentes Principais

<br>

A Análise de Componentes Principais (ACP) é uma técnica de Análise Multivariada que consiste em transformar um conjunto original de variáveis em outro conjunto, os Componentes Principais (CP) com propriedades específicas. Os CP’s são combinações lineares das variáveis originais e são estimados de forma a captar o máximo da variação total dos dados. O processo de estimação é tal que o primeiro CP capta o máximo de variância possível, o segundo capta o máximo possível do restante de variância, o terceiro o máximo possível do restante de variância, e assim sucessivamente.

A ACP é apropriada quando as variáveis sob investigação são todas de mesma natureza, de modo que não tenhamos, por exemplo, uma ou mais variáveis dependentes e um conjunto de covariáveis, como no caso de análise de regressão.

Segundo Mingoti (2005, p. 59), *"seu objetivo principal é o de explicar a estrutura de variância-covariância de um vetor aleatório, composto de p-variáveis aleatórias, através da construção de combinações lineares das variáveis originais. Estas combinações lineares são chamadas de componentes principais e são não correlacionadas entre si"*.

Uma combinação linear de vetores ou de variáveis é um novo vetor (ou nova variável) defindo por 

$$
y=a_1x_1+a_2x_2+a_3x_3+ \dots + a_px_p
$$
*"Se temos p-variáveis originais é possível obter-se p componentes principais. No entanto, em geral deseja-se obter 'redução do número de variáveis a serem avaliadas e interpretação das combinações lineares construídas', ou seja, a informação contida nas p-variáveis originais é substituída pela informação contida em k (k$<$p) componentes principais não correlacionados"* (MINGOTI, 2005, p. 59).

A ideia é que se algumas das variáveis originais são correlacionadas, elas estão, efetivamente, “dizendo a mesma coisa”. Nesse caso, um conjunto menor de variáveis, não-correlacionadas, pode ser tão eficaz quanto o conjunto de variáveis originais para explicar a estrutura de variância-covariância dos dados.

Matricialmente, pode ser escrita como $y=a'x$ onde os a's são as constantes que definem a combinação linear e são determinadas de forma a atender às características da combinação linear que se deseja. Como as variáveis são dadas, os coeficientes a's são determinados de forma a atender às restrições estabelecidas, ou seja, o princípio da técnica. As combinações lineares tem média e variância definidas, respectivamente, por $E(Y)=a'\mu$ e $V(Y)=a'\sum a$. 

Mais de uma combinação linear pode ser definida de um conjunto de variáveis. Em geral, com *p* variáveis pode-se formar p combinaçoes lineares diferentes. 

A ACP transforma um conjunto de variáveis correlacionadas em um conjunto de variáveis não-correlacionadas. Assim, se as variáveis originais são aproximadamente não correlacionadas, não faz sentido ser feita uma ACP.

Variáveis quantitativas usadas em análise multivariada são, geralmente, expressas em unidades diferentes. Diferenças de escalas afetam a contribuição da variável para a variância generalizada. Para usar essas variáveis em uma técnica multivariada elas precisam ser transformadas para uma escala comum. Entre os métodos usados alguns eliminam diferenças em tamanho (escala) outras reduzem tamanho e variabilidade para uma escala comum.

O método mais usado para tornar variáveis comparáveis é a *Padronização*, que consiste em subtrair a média e dividir pelo desvio padrão. Além de simplificar cálculos e manipulações matemáticas, a padronização de variáveis é importante para resolver o problema de unidades de medidas diferentes das variáveis e do desbalanceamento entre as variâncias. Toda variável padronizada tem média zero, variância igual a um e é adimensional. 

Uma questão importante é que a covariância entre variáveis padronizadas é igual a correlação entre variáveis originais. Assim, usar a matriz de correlações das variáveis ao invés da matriz var-cov é o mesmo que trabalhar com variáveis padronizadas. 

Existe uma tendência para que a variável com maior volatilidade cause uma desestabilização na Análise de Componentes Principais e também na Análise Fatorial. A solução é padronizar as variáveis de forma que tenham média zero e variância unitária.

A Análise passa a ser determinar as *raízes características* e os *vetores característicos* da matriz de correlações. 

Sobre raízes e vetores característicos, uma matriz quadrada A tem raízes características $\lambda_i$ e vetores característicos $X_i$ dados pela seguinte relação $AX=\lambda X$. Uma matriz $p_xp$ tem *p* raízes e *p* vetores característicos e a relação pode ser escrita como $AX_i=\lambda_i X_i$ o que, intituitivamente, significa que existem constantes $\lambda_i$ e vetores $X_i$, tais que a multiplicação da matriz pelo vetor é igual à multiplicação do vetor por uma constante. 

As raízes e vetores característicos são encontrados a partir de 

$$
AX=\lambda X
$$

$$
AX - \lambda X= 0
$$
$$
(A- \lambda I)X= 0
$$

sendo que a equação é verdadeira para qualquer $\lambda$ se $X=0$, mas esta solução não interessa. Para se ter uma solução $X \neq 0$, a inversa da equação característica $(A- \lambda I)$ não deve existir e, para isto, o seu determinante precisa ser igual a zero. Assim, pode-se entender $\lambda$ (raízes características) como os valores que zeram o determinante da equação característica.

Os vetores característicos não são únicos, devendo ser normalizados. Para cada raiz característica existe um vetor característico que é encontrado resolvendo a expressão $(A-\lambda_i I)X_i=0$. 

As raízes características possuem um conjunto de propriedades: 

1) $tr(A) =  \sum_{i=1}^{p} \lambda_i$;
2) $det(A) = \Pi_{i=1}^{p}$. Se alguma raiz característica for zero, o determinante de A é zero e a matriz não possui inversa; 
3) Para uma matriz diagonal, as raízes são os elementos da diagonal principal;
4) Para uma matriz triangular, as raízes são os elementos da diagonal principal;
5) Raízes de A são iguais as de $A'$;
6) As raízes de $A^{-1}$ são iguais a $1/ \lambda_i$, mas os vetores característicos são os mesmos;
7) Se A é ortogonal, $\lambda_i=1$ ou $-1$;
8) Se A é uma matriz idempotente, $\lambda_i=1$ ou $0$;
9) Raízes características de matrizes simétrica;
10) Os vetores característicos de matrizes simétricas são ortogonais, ou seja, não
correlacionados.

<br>

### Obtenção Componentes Principais

<br>

Considere um vetor de p variáveis padronizadas dada por

$$
\mathbf{Z'}=\left[\begin{array}{llll}
Z_1 &Z_2 &\dots &Z_p
\end{array}\right]
$$
os CP's são combinações lineares dos $Z's$

$$
Y_j=a'_jZ=a_{j1}Z_1+a_{j2}Z_2+\dots+a_{jp}Z_p
$$

sendo que é possível ter até *p* CP's, com o primeiro tendo a maior variância, o segundo CP tendo a segunda maior variância e sendo ortogonal ao primeiro, etc. Neste caso, a variância de $Z_i$ é dada por $V(Z_i)=a_i'\sum a_i$ e o que deve ser feito é encontrar os coeficientes de $a_i$ das combinaçoes lineares de forma a satisfazer as condições acima definidas. 

É possível demonstrar (LIMA, 2015) que a solução se resume em encontrar as raízes características $(\lambda_i)$ e os vetores característicos $(a_i)$ da matriz var-cov $(\sum)$ das variáveis. Os coeficientes dos CP's são os elementos dos vetores característicos $(a_i)$. 

Além disso, é possível demonstrar que $\sum a_i= \lambda a_i$ e, consequentemente, 

$$
V(Z_i)=a_i'\sum a_i = a_i'\lambda a_i = \lambda a_i'a_i 
$$

e que a $V(Z_i) = \lambda _i$ pois pela ortogonalidade $a_i'a_i =1$ ($a_i'a_i=\sum_{p=1}^ja_{jp}^2$), o que significa que a variância do componente i é igual a sua raiz característica. 

Finalmente, dado que $a_i'a_k =0$, a covariância entre os CP's é igual a zero, ou seja, não são correlacionados, são ortogonais. 

Como dito anteriormente, com *p* variáveis é possível ter até *p* CP's. Assim, a variância total dos *p* componentes tem que ser igual a variância total das variáveis Z. 

A importância relativa de cada componente é dada pelo percentual de sua variância em relação à variância total, ou seja, é a variância explicada ou captada por ele. Assim, a importância relativa de $Z_k$ é dada por $( \lambda_k / \sum_{i}^{p} \lambda_i)100$. Se um, dois ou três componentes captam grande parcela da variância dos dados, pode-se concentrar a análise neste número menor de variáveis.

Como a idéia da ACP é redução da massa de dados para uma dimensão mais adequada para análise, se faz necessário decidir quantos CP’s usar. Com p variáveis deve-se manter k componentes, sendo $k<q$. A determinação de k não é uma decisão estatística porque não se tem um modelo adequado para tal e por isso é feita de forma prática. Utiliza-se os seguintes critérios:

a) Dada a importância relativa de cada componente, manter o número de componentes que captam "certa" percentagem da variância dos dados, com $70\%$ sendo um valor de referência;

b) Desconsiderar os componentes com variância inferior à variância média das variáveis originais.

<br>

### Interpretação dos Coeficientes do Componente Principal

<br>

Os coeficientes dos componentes indicam a importância da variável para o componente. Isto possibilita atribuir um significado ao componente. Além do coeficiente pode-se calcular a correlação entre o componente e a variável. A correlação entre $Y_j$ e a variável padronizada $Z_i$ é igual a:

$$
r_{Y_i,Z_k}=\frac{a_{ik}}{\sqrt{\sigma_k^2}}\sqrt{\lambda_i}
$$
e, então, as variáveis $\mathbf{Z}$ com os maiores coeficientes na componente principal $\mathbf{Y_j}$ são as mais correlacionadas com a componente. A matriz de correlações entre as variáveis e os componentes é bastante
importante para entender os componentes e lhes atribuir um nome. É possível também testar a significância estatística de cada peso ou coeficiente do componente. A hipótese nula é de que o coeficiente é estatisticamente igual a zero.

<br>

### Análise de Componentes Principais - Escores

<br>

Escore é o valor de $\mathbf{Y_i}$ para cada observação. Estes servem para, por exemplo:

I) comparar ou ordenar as observações; 
II) Análise de cluster ou regressão.

<br>

### Demonstração de ACP no R

<br>

```  {r estat2, warning=FALSE, message=FALSE}
#Direcionado o R para o Diretorio a ser trabalhado
#setwd('/Users/jricardofl/Dropbox/tempecon/multivariada')

#Lendo os dados no R
library(car)
library(tidyverse)
library(corrplot)
library(graphics)
library(ade4)
library(grid)
library(MVar.pt)
library(factoextra)

data(mtcars)

#Format
#A data frame with 32 observations on 11 (numeric) variables.

#[, 1] mpg Milhas/(EUA) galão
#[, 2] cil Número de cilindros
#[, 3] disp Cilindradas cc
#[, 4] HP Potência bruta
#[, 5] drat Relação do eixo traseiro
#[, 6] Peso em peso (1000 lbs)
#[, 7] qsex tempo 1/4 de milha
#[, 8] vs Motor (0 = em forma de V, 1 = reto)
#[, 9] am Transmissão (0 = automática, 1 = manual)
#[,10] gear Número de marchas para frente
#[,11] carb Número de carburadores

# Criando um objeto
dados <- mtcars
summary(dados) # estatística descritiva dos dados

# Retirando as variáveis binárias
dados <- mtcars %>% select(-vs, -am)

# Verificação da estrutura dos dados
str(dados)
head(dados)
tail(dados)

# ESTIMACAO DAS ESTATISTICAS DESCRITIVAS E CALCULO DA MATRIZ DE CORRELAÇOES
summary(dados) #sem as variáveis binárias
desvpad <- sapply(dados, sd) #para calcular o desvio-padrao
round(desvpad,2)
cov(dados) #covariância

# Matriz de Correlacoes
matcor <- cor(dados)
print(matcor, digits=4)
corrplot(cor(dados), order="hclust", tl.col="black", tl.cex = .75)

#Análise de componentes principais - dados devem ser padronizados
#Padronizacao dos dados
dados.pad <-as.data.frame(scale(dados))

#Analise de Componentes principais com prcomp
resultados.pca <- prcomp(dados.pad, scale = TRUE) #ACP

#Raizes Caracteristicas - 1 Forma
(resultados.pca$sdev)^2

summary(resultados.pca)

sum((resultados.pca$sdev)^2) #traco da matriz

#É possivel escolher o número de CP com base no Autovalor >1
(resultados.pca$sdev^2)

# Gráfico do percentual de variância explicada
screeplot(resultados.pca, type="lines")
fviz_eig(resultados.pca)

```

<br>

Se observa que 62,84% da variabilidade total dos dados são explicados pela primeira componente
principal. Além disso, a segunda componente principal explica 23,13% da variação total. As duas primeiras componentes principais explicam juntas 85,98% da variabilidade total dos dados. 

<br>

```  {r estat3, warning=FALSE, message=FALSE}
#Resultados das combinações lineares
resultados.pca

#Escores para cada observação
resultados.pca$x # valores das componentes principais

#Correlaçoes entre as variaveis e os escores
cor(dados, resultados.pca$x)
```

<br>

### Análise Gráfica da ACP

<br>

```  {r estat4, warning=FALSE, message=FALSE}
fviz_pca_ind(resultados.pca,
             col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE, 
             legend.title = "Representation")

fviz_pca_var(resultados.pca, 
             col.var = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE, 
             legend.title = "Contribution")

fviz_pca_biplot(resultados.pca, 
                repel = TRUE, 
                col.var = "#2E9FDF", 
                col.ind = "#696969")

dados <- mtcars %>% mutate(
  am = case_when(am == 0 ~ "Automatic", 
                 TRUE ~ "Manual"))

fviz_pca_ind(resultados.pca, 
            col.ind = dados$am, 
             palette = c("#00AFBB", "#FC4E07"), 
             addEllipses = TRUE, 
             legend.title = "Engine shape", 
             repel = TRUE)

fviz_pca_biplot(resultados.pca, 
                repel = TRUE, 
                col.var = "black", 
                col.ind = as.factor(dados$am), 
                addEllipses = TRUE, 
                legend.title = "Transmissão")
```

<br>

## Análise Fatorial

<br>

*Análise Fatorial* é uma técnica estatística para redução de dados. Ela reduz o número de variáveis (p) na análise, descrevendo combinações lineares destas (fatores) que contém a maior parte das informações das variáveis originais e que possam ter interpretações significativas. Estes fatores não são correlacionados entre si.

O "fator" é uma variável latente (não observada) que representa uma característica marcante dos dados. O objetivo da *Análise Fatorial* é identificar os r<p fatores e relacioná-los com as variáveis originais.

Segundo Mingoti (2005, p. 99), "a análise fatorial tem como objetivo principal descrever a variabilidade original do vetor de variáveis X, em termos de um número menor r de variáveis aleatórias, chamadas de fatores comuns e que estão relacionadas com o vetor original X através de um modelo linear. Neste modelo, parte da variabilidade de X é atribuída aos fatores comuns, sendo o restante da variabilidade de X atribuído às variáveis que não foram incluídas no modelo, ou seja, ao erro aleatório". 

Existem dois enfoques diferentes para *Análise Fatorial*: A **Análise Fatorial Exploratória** e a **Análise Fatorial Confirmatória**. A **Análise Fatorial Exploratória** objetiva determinar dimensões latentes dos dados denominadas fatores partindo-se de um conjunto de variáveis e a solução conduz a uma relação de todas as variáveis com todos os fatores. Na **Análise Fatorial Confirmatória** parte-se de um conjunto de variáveis e de um conjunto de hipóteses sobre o número de fatores e sobre quais variáveis se relacionam com quais fatores. O objetivo da análise é "confirmar" se as variáveis formam os fatores da forma como foi assumido. A formulação do modelo é baseada em uma teoria que será testada com a análise. O modelo especifica quais variáveis são relacionadas com quais fatores e se os fatores são correlacionados. A definição dos fatores é feita antes do ajustamento do modelo.

Seja $\mathbf{x}$ um vetor aleatório $p_x1$ com média $\mathbf{\mu}$ e matriz de variância-covariância dada por $\mathbf{\Sigma}$. A variância total de cada variável pode ser separada em três parcelas: fatores comuns (influenciam duas ou mais variáveis), fatores específicos (contribuem para a variação de uma única variável) e um erro. 

Assim, a variação total vai ser a soma da variação comum (comunalidade), da variação específica (unicidade) e do erro. 

$$
\mathbf{x}=\mathbf{\Lambda} \mathbf{f}+ \mathbf{\epsilon}
$$

onde $\mathbf{f}$ é um vetor aleatório de ordem $k_x1$($k<p$), com os elementos $f_1, \dots, f_k$ sendo denominados fatores comuns (comunalidade); $\Lambda$ é uma matriz de constantes desconhecidas $p_xk$ chamadas de cargas fatoriais (ou coeficientes de correlação entre as variáveis e os fatores quando as variáveis são padronizadas); e os elementos $\epsilon_1, \dots, \epsilon_p$ do $p_x1$ vetor aleatório $\mathbf{\epsilon}$ são chamados de fatores específicos (unicidade) e erro. Assume-se que  $\mathbf{f}$ e $\mathbf{\epsilon}$ não são correlacionados.

\begin{cases}
X_1=a_{11}F_1+a_{12}F_2+a_{13}F_3+ \dots +a_{1r}F_r+\epsilon_1 \\
X_2=a_{21}F_1+a_{22}F_2+a_{23}F_3+ \dots +a_{2r}F_r+\epsilon_2 \\
X_3=a_{31}F_1+a_{32}F_2+a_{33}F_3+ \dots +a_{3r}F_r+\epsilon_3 \\
. \\
. \\ 
. \\
X_p=a_{p1}F_1+a_{p2}F_2+a_{p3}F_3+ \dots +a_{pr}F_r+\epsilon_p \\
\end{cases}

O modelo tem como objetivo explicar o comportamento das p variáveis em função de r<p fatores comuns (desconhecidos) e de um termo de erro composto de unicidade (fatores específicos) e erro aleatório. O processo de estimação que consiste em determinar a matriz de cargas fatoriais $\mathbf{\Lambda}$.

<br>

### Pressupostos do Modelo de Análise Fatorial
<br>

Os pressupostos do Modelo de Análise Fatorial podem ser resumidos em:

1. $\mathbf{f} \sim (\mathbf{0},\mathbf{I})$, ou seja, os fatores possuem média zero e variância constante igual a um e não são autocorrelacionados;

2. $\epsilon \sim (\mathbf{0},\mathbf{\Psi})$, em que $\mathbf{\Psi}=diag(\psi_1,\dots,\psi_p)$, ou seja, os erros tem média zero e podem ter variâncias diferentes, mas não correlacionados;
	 
3. $\mathbf{f}$ e $\mathbf{\epsilon}$ são independentes, ou seja, os fatores comuns são independentes dos fatores específicos e erros;

A Análise Fatorial pode ser feita com a matriz de variâncias e covariâncias ou com a matriz de correlações. Como normalmente é recomendado o uso de variáveis padronizadas para contornar o problema de unidades de medidas diferentes e a influência que uma variável com variância grande pode ter na determinação das cargas fatoriais, a Análise Fatorial é, quase sempre, feita com a matriz de correlações $\mathbf{\Sigma}$ .

No modelo fatorial, a matriz $\mathbf{\Sigma}$ é decomposta como

$$
\mathbf{\Sigma}=E(xx')
$$

$$
\mathbf{\Sigma}=E[(\mathbf{\Lambda} \mathbf{f}+ \mathbf{\epsilon})(\mathbf{\Lambda} \mathbf{f}+ \mathbf{\epsilon})']
$$

sendo que é possível demonstrar que o resultado desta decomposição é 

$$
\mathbf{\Sigma}=\mathbf{\Lambda}\mathbf{\Phi}\mathbf{\Lambda}'+\mathbf{\Psi}
$$
Este resultado, que é derivado com base nas pressuposições do modelo, diz que a matriz de correlações pode ser decomposta em duas parcelas, uma relacionada com a comunalidade e outra com a unicidade. O modelo assume que os fatores não são correlacionados $\mathbf{\Phi}=\mathbf{I}$. 

Um termo importante na Análise Fatorial é a comunalidade, ou seja, o somatório das correlações dos fatores com a variável "**i**".

$$
\mathbf{h_i^2}=a_{i1}^2+a_{i2}^2+\dots+a_{ir}^2
$$

A Unicidade ($\mathbf{\Psi}$) é dada por $\mathbf{\Psi}=1-\mathbf{h_i^2}$. 

A comunalidade é a parcela da variância de X explicada pelos r fatores e a unicidade é a parcela não explicada. 

<br>

### Estimação das Cargas Fatoriais

<br>

Dado o modelo 

$$
\mathbf{x}=\mathbf{\Lambda} \mathbf{f}+ \mathbf{\epsilon}
$$
pode-se entender o x como a variável dependente e o f como as variáveis explicativas, $\Lambda$ como como os coefientes e $\epsilon$ como os erros de um modelo de regressão múltipla. 

O problema, então, é estimar $\mathbf{\Lambda}$ e $\mathbf{\Psi}$ que reproduzam $\mathbf{\Sigma}$ com um número de fatores r menor que o número de variáveis originais p, mas apenas o x é conhecido.

Existem diversas formas de estimar as cargas fatoriais sendo que componentes principais e  máxima verossimilhança são as principais. As explicações mais detalhadas podem ser encontradas em Mingoti (2005).

O método dos **Componentes Principais** é o mais usado e tem como base o uso das raízes características e vetores característicos relacionados com r<p componentes para estimar $\Lambda$. O método de **Máxima Verossimilhança** maximiza uma função de verossimilhança formada com a pressuposição de que o vetor de variáveis aleatórias X segue distribuição normal p-variada com vetor de médias $\mu$ e matriz de variâncias e covariâncias $\mathbf{\Sigma}$.

O método dos componentes principais parte da *decomposição espectral* da matriz de variâncias e covariâncias $\mathbf{\Sigma}$. Assim, é necessário informar que a *decomposição espectral* de uma matriz é uma operação que relaciona a matriz com seus autovalores e seus autovetores. 

Considere A uma matriz $p_xp$ simétrica com raízes características $\lambda_p$ e vetores característicos $x_p$. A decomposição espectral de A é dada por

$$
A=\sum \lambda_i x_i x_i'
$$
com i variando de 1 a p. A decomposição pode ser representada por $A=P \Lambda P'$ em que P é uma matriz cujas colunas são os vetores característicos normalizados de A e $\Lambda$ é uma matriz diagonal com as raízes características de A na diagonal principal. Dessa relação tem-se, também, que $\Lambda=PAP'$ que é uma operação denominada diagonalização de A.

Pelo teorema da decomposição espectral, a matriz de correlação amostral ou a matriz de variâncias e covariâncias pode ser decomposta como a soma de p matrizes, cada uma relacionada com um autovetor da matriz $\mathbf{\Sigma}$.

$$
\Sigma=P \Lambda P'=\sum \lambda_i x_i x_i'
$$
com i variando de 1 a p. Com p componentes, a matriz $\mathbf{\Sigma}$ é totalmente reproduzida. Para r < p, pode-se escrever 

$$
\Sigma=P \Lambda P'=\sum_{j=1}^r \lambda_j x_j x_j' + \sum_{j=r+1}^p \lambda_j x_j x_j'
$$

Cada parcela desta soma envolve uma matriz de dimensão $p_xp$ correspondente a informação da j-ésima componente fazendo com que a variabilidade das variáveis seja representada pela soma da variabilidade relacionada com cada componente. Assim, pode-se escrever


$$
\Sigma=\sum_{j=1}^r \lambda_j x_j x_j' + \sum_{j=r+1}^p \lambda_j x_j x_j'=P_1 \Lambda_1 P_1'+P_2 \Lambda_2 P_2'
$$
e pode-se considerar uma aproximação da matriz de correlações $\mathbf{\Sigma}$ dada por

$$
\mathbf{\Sigma} \approx P_1 \Lambda_1 P_1'=\sum_{j=1}^r \lambda_j x_j x_j'
$$
o que possibilita estimar as matrizes $\Lambda$ e $\Psi$ por meio de raízes e vetores característicos de $\mathbf{\Sigma}$.

Pela decomposição espectral ainda é possível escrever 

$$
\mathbf{\Sigma} \approx P_1 \Lambda_1^{1/2} \Lambda_1^{1/2}P_1'
$$
e definir por $A=P_1 \Lambda_1^{1/2}$ e por $A'=\Lambda_1^{1/2}P_1'$

Esta aproximação de $\mathbf{\Sigma}$ considera que os fatores específicos são de menor importância. Incluindo os fatores específicos a aproximação de $\mathbf{\Sigma}$ fica 

$$
\mathbf{\Sigma} \approx AA' + \Psi
$$
e a matriz $\Psi$, que é diagonal, pode ser estimada por 

$$
\Psi = diag(R-AA')
$$

A matriz de resíduos resultante do ajustamento do modelo é definida por

$$
RES= \mathbf{\Sigma} - (AA'+\Psi)
$$

que serve como critério de avaliação do modelo. Valores pequenos, próximos de zero, indicam bom ajustamento. Esta matriz só é nula quando todos os "p" fatores são extraídos, o que não é o desejado na prática. Através destes procedimentos os elementos da diagonal da matriz $\mathbf{\Sigma}$ (variâncias) são exatamente reproduzidos por $AA'+\Psi$. Entretanto, o mesmo não ocorre para os elementos fora da diagonal principal (correlações).

Todo o conhecimento de Componentes Principais se aplica neste caso com os componentes agora denominados fatores.

<br>

### Demonstração de Análise Fatorial no R por Componentes Principais

<br>

```  {r estat5, warning=FALSE, message=FALSE}
#Direcionado o R para o Diretorio a ser trabalhado
setwd('/Users/jricardofl/Dropbox/tempecon/multivariada')

library(tidyverse)
library(skimr)
library(psych)
library(REdaS)

#Lendo os dados no R
dados <- read.csv2('winequality-red.csv', sep=";", dec=".")
dados <- dados %>% 
  select(-quality) #retira a variável quality

attach(dados)
glimpse(dados)

head(dados)
tail(dados)

# Estatística descritiva das variáveis
summary(dados)
skim(dados)
```

O banco de dados é composto por 1599 observações e 12 variáveis: acidez fixa, acidez volátil, ácido cítrico, açucar residual, cloretos, dióxido de
enxofre livre, dióxido de enxofre total, densidade, Ph, sulfatos, álcool.

```  {r estat6, warning=FALSE, message=FALSE}
library(corrplot)
corrplot(cor(dados), order="hclust", tl.col="black", tl.cex = .75)

#Obtenção do Modelo Fatorial Ortogonal

#Analise Fatorial - dados devem ser padronizados
#Padronizaçao dos dados
dados.pad <-as.data.frame(scale(dados))

s <- cov(dados.pad) #matriz de cov com var pad = mat correlaçoes origem

#Autovalores e Autovetores da Matriz de Covariancias
lambda <-eigen(s)$values
lambda #
evec <-eigen(s)$vectors
evec
evec[,1:4]

#Matriz de Cargas Fatoriais A modelo completo
A <- sqrt(lambda)*t(evec)
A <- t(A) #Extracao dos Fatores
A[,1:4] #4 primeiros fatores
AAT <- A%*%t(A)
AAT

#Proporção da variabilidade explicada por cada componente
round(lambda/sum(lambda),4)

#Matriz de Cargas Fatoriais m =4
Am4 <- A[,1:4]
AATm4 <- Am4%*%t(Am4)
AATm4

#Matriz PSI - Unicidade
psi <- diag(diag(s-AATm4))
psi

#Matriz Residual
s-(AATm4+psi)
```

```{r estat7, warning=FALSE, message=FALSE}
#2 Forma de fazer a Analise Fatorial
matcor <- cor(dados)

k <- 4
dados.pca <- prcomp(dados.pad, scale = TRUE) #PCA
carfat <- dados.pca$rotation[, 1:k] %*% diag(dados.pca$sdev[1:k])
colnames(carfat) <- paste("Fator", 1:k, sep = " ")

#Cargas Fatoriais com autovalor maior do que 1
carfat

#Comunalidade e Unicidade
comum <- rowSums(carfat^2)
vespec <- diag(matcor) - comum
estimat <- cbind(comum, vespec, diag(matcor))
rownames(estimat) <- colnames(dados)
colnames(estimat) <- c("Comunalidade", "Unicidade", "Variância")
estimat
#Matriz de Resíduos
resid <- matcor - (carfat %*% t(carfat) + diag(vespec))
resid

# Estimativas das cargas fatoriais das variaveis 

plot(carfat, pch = 20, col = "red", xlab = "Fator 1", ylab = "Fator 2")
text(carfat, rownames(carfat), adj = 1)
```

Em relação a Análise Fatorial por máxima verossimilhança, supondo o vetor de variáveis com distribuição normal p-variada, tem-se

a) as variáveis padronizadas serão Z $\sim$ N(0, $\Sigma$);

b) o vetor de fatores será F $\sim$ N(0,I)

c) o vetor de erros será $\epsilon \sim N(0, \Psi)$

Pelo modelo fatorial $\mathbf{x}=\mathbf{\Lambda} \mathbf{f}+ \mathbf{\epsilon}$ e $\mathbf{\Sigma} \approx AA' + \Psi$. Com uma amostra de n observações o objetivo é estimar $\hat A$ e $\hat \Psi$. O procedimento consiste em maximizar a *função de verossimilhança* 

$$
L(0, \Sigma)=(2 \pi)^{(\frac{-np}{2})}|AA' + \Psi|^{\frac{-n}{2}}e^{\frac{-1}{2}\sum_{j=1}^n Z_j'(AA'+\Psi)^{-1}Z_j}
$$
É necessário definir o número de fatores antecipadamente. Uma mudança neste número acarreta mudança nas cargas fatoriais, diferentemente do método de componentes principais. Na análise fatorial por máxima verossimilhança há equivalência entre decompor $\Sigma$ ou a matriz de covariâncias $S$, o que não ocorre na AF via componentes principais. 

<br>

### Demonstração de Análise Fatorial no R por Máxima Verossimilhança

<br>

```{r estat8, warning=FALSE, message=FALSE}

# Definição do número de Fatores

eigv <- eigen(cor(dados))
eigv <- data.frame(nfact = 1:ncol(dados), eigval = eigv$values)
ggplot(data = eigv, mapping = aes(nfact, eigval)) +
geom_line() +
geom_point() +
geom_abline(slope = 0, intercept = 1, color = "red") +
labs(x = "Número de fatores",
y = "Autovalor",
title = "Scree plot") +
theme_bw()

#Terceira forma de fazer - factanal()
#Usar as variaveis padronizadas - dados.pad
#Não usa componentes principais, usa max verosimilhança

#Pelo Screeplot, tem-se 4 autovalores acima de 1

fatorial1 <- factanal(dados.pad, factors=4, rotation="none", na.action=na.omit)

fatorial1

#fatorial1$loadings
load = fatorial1$loadings[,c(1,2)]
plot(load, type="n")
text(load, labels=names(dados.pad),cex = .7)#visualiza variaveis com fatores

fa.diagram(fatorial1$loadings, digits = 3, main = "Análise Fatorial")
```

A partir do diagrama com as cargas fatoriais é possível observar que as variáveis cloreto e sulfatos têm pouco peso na composição dos fatores. Assim, pode-se excluir tais variáveis do modelo.

```{r estat9, warning=FALSE, message=FALSE}
dados2 <- subset(dados, select = -c(sulphates, chlorides))

dados2.pad <- as.data.frame(scale(dados2))

fatorial2<- factanal(dados2.pad, factors=4, rotation="none", na.action=na.omit)

fatorial2
```

<br>

### Rotação de Fatores

<br>

Sabe-se que vetores característicos não são únicos e, por isso, as cargas fatoriais da Análise Fatorial por Componentes Principais podem ser modificadas sem prejudicar o significado da análise. A rotação Consiste em "modificar" as cargas fatoriais, ou seja, calcular nova matriz A. O objetivo é obter uma matriz de cargas fatoriais de mais fácil interpretação, onde cada fator se relaciona mais distintamente com certo grupo de variáveis.

No entanto, nem sempre se tem uma estrutura nítida de relacionamento de variáveis e fatores. Normalmente, todas as variáveis apresentam coeficiente de correlação de certa magnitude com todos os fatores e, muitas vezes, é difícil identificar a relação de forma adequada. A rotação de fatores/eixos é um procedimento matematicamente correto e tem como finalidade facilitar a interpretação dos fatores, isto é, gerar uma nova solução para as cargas fatoriais que mostra uma relação mais nítida entre variáveis e fatores.

A rotação dos fatores consiste na rotação dos eixos coordenados e o cálculo de novos valores de abscissas e ordenadas relacionados com o novo sistema de eixos. Se o ângulo do novo sistema se mantiver em 90º, a rotação é denominada **ortogonal** e se mudar, a rotação é denominada **oblíqua**. No primeiro caso, os fatores permanecem não correlacionados, mas, no segundo caso, haverá correlação o que dificulta interpretação.

Vale ressaltar que a rotação ortogonal modifica as cargas fatoriais, mas não modifica as comunalidades ($h_i^2$) e, como observa Mingoti (2005), “em termos de qualidade de ajuste, esta nova solução não acrescenta nenhuma melhoria em relação ao ajuste obtido usando a matriz 2ihA, pois a matriz residual original não é alterada pela transformação ortogonal”.

Existem diversos métodos de rotação, tanto ortogonal (varimax, quartimax, orthomax, equimax) quanto oblíqua (oblimim, quartimim, biquartimim, covax). Entretanto, o método de rotação mais utilizado é o Varimax, o qual permite que os coeficientes de correlação entre os indicadores e os fatores fiquem o mais próximo possível de zero ou de 1 em valor absoluto, facilitando a interpretação.

O método de Rotação Varimax "forma um novo sistema de eixos ortogonais com o mesmo número de fatores e permite que o grupo de variáveis apareça com maior nitidez, facilitando a interpretação e Análise" (ZAMBRANO e LIMA, 2004). 

Considere que partindo da já vista matriz de correlaçoes

$$
\mathbf{\Sigma} = AA'+\Psi
$$
se tenha uma matriz ortogonal T de tal forma que $TT'=I$. Então, é possível escrever 

$$
\mathbf{\Sigma} = ATT'A'+\Psi
$$

$$
\mathbf{\Sigma} = AT(AT)'+\Psi
$$

se denominar AT por A*, tem-se

$$
\mathbf{\Sigma} = A^{*} A^{* '}+ \Psi
$$
isto significa que dada uma solução para A, é possível encontrar uma outra solução para $A^*$, através da escolha da matriz ortogonal T, que seja de mais fácil interpretação do que a solução original. 

No critério Varimax, a busca pela matriz T tem como base a tentativa de encontrar fatores com grandes variabilidades nas cargas fatoriais, isto é, encontrar para um fator fixo, um grupo de variáveis X altamente correlacionadas com o fator e um outro grupo de variáveis que tenham correlação desprezível ou moderada com o fator. 

Para cada fator fixo, a solução é obtida através da maximização da variação dos quadrados das cargas fatoriais originais das colunas da matriz A. Seja $\hat a_{ij}^*$ o coeficiente da i-ésima variável no j-ésimo fator após a rotação. Seja V a quantidade definida por

$$
V= \frac{1}{p} \sum_{j=1}^m \Bigg[\sum_{i=1}^p \tilde a_{ij}^4-\frac{1}{p}(\sum_{i=1}^p \tilde a_{ij}^2)^2 \Bigg]
$$

onde $\tilde a_{ij}=(\hat a_{ij}^*/ \hat h_i)$, sendo $\hat h_i$ a raiz quadrada da comunalidade da variável $X_i$. A maximização de V corresponde a "puxar" os quadrados das cargas sobre cada fator o máximo possível. O que se espera é encontrar grupos definidos de coeficientes para cada coluna de fator.

A função da rotação de fatores tem a função de procurar cargas fatoriais que mostram um padrão de relacionamento claro e de fácil interpretação entre variáveis e fatores. Deve-se observar que as comunalidades não mudam, o que é uma característica da rotação ortogonal.


```{r estat10, warning=FALSE, message=FALSE}

#Esimação da AF fazendo a rotação Varimax
fatorial3<- factanal(dados2.pad, factors=4, rotation="varimax", na.action=na.omit)

fatorial3

fa.diagram(fatorial3$loadings, digits = 3, main = "Análise Fatorial - Modelo Rotacionado Varimax")

#Comunalidades
rowSums(fatorial3$loadings^2)


# Matriz Residual
L <- fatorial3$loadings

rho_til <- L%*%t(L)+diag(fatorial3$uniquenesses)
U <- cor(dados2.pad) - rho_til
round(U, 4)
```

Para esse último modelo, o fator 1 está fortemente relacionado à acidez e Ph. O fator 2 ao dióxido de enxofre, o fator 3 é composto pela
quantidade de alcool e o fator 4 pela densidade e açucar residual.

A partir das comunalidades é possível concluir que os 4 fatores explicam cerca de 92% da variabilidade da acidez fixa, pouco mais de 19% da
variabilidade da acidez volátil, 57% da variabilidade da acidez cítrica, 30% da variabilidade do açucar residual, 51% da variabilidade do enxofre livre, 88% da variação no enxofre total, 99% da variação na densidade, 70% na variação do Ph e 99% da variabilidade do teor alcólico.

A análise da matriz residual indica a qualidade do modelo, que deve possuir uma matriz residual com valores muito próximos a zero.
Assim, a partir desta matriz residual, o modelo em questão indica que a Análise Fatorial se ajusta bem aos dados amostrais, pois a grande maioria dos valores da matriz foram calculados proximos a zero.

<br>	 

### Escores Fatoriais

<br>

Escores fatoriais são os valores de cada fator para cada observação da amostra. São importantes para mapeamento das observações e para serem utilizadas em outras técnicas, como cluster, regressão, etc. Como se tem um modelo estatístico estes escores devem ser estimados à semelhança de um modelo de regressão onde se obtém previsões para a variável dependente. 

```{r estat11, warning=FALSE, message=FALSE}

#Esimação da AF fazendo a rotação Varimax
fatorial3<- factanal(dados2.pad, factors=4, rotation="varimax", na.action=na.omit, scores = "regression")

escores <- as.data.frame(fatorial3$scores)

head(escores)
tail(escores)

ggplot(data = escores, mapping = aes(x = Factor1, y = Factor2)) +
geom_point() +
labs(x = "Fator 1",
y = "Fator 2",
title = "Dispersão dos escores fatoriais") +
theme_bw()

```
<br>	

### Análise da Adequabilidade 

<br>

1. **Matriz de Correlações**: examinar a matriz de correlações simples, procurando visualizar algum padrão de relacionamento entre as variáveis; devem existir grupos de variáveis correlacionadas;

2. **Matriz Anti-Imagem**: Matriz de correlações Parciais com sinais invertidos. Matrizes anti-imagem podem ser usadas para avaliar se variáveis individuais devem ser incluídas na análise fatorial. Isso significa trazer a porção de variância de uma variável que pode ser explicada com as variáveis correlacionadas (imagem) em associação com a porção de variância inexplicável (anti-imagem). As variáveis são adequadas para incluir na análise fatorial se os valores da matriz anti-imagem forem baixos.

3. **Teste de esfericidade de Bartlett**: testa se a matriz de correlações é estatisticamente igual a uma matriz identidade. Se for, não é boa para Análise Fatorial. O que se busca, então, é rejeitar a hipótese nula de que a matriz de correlação verdadeira é uma matriz identidade;
	
4. **Medida KMO (Kayser-Meyer-Olkin)**: é um índice que compara correlações simples e parciais:

$$
\mathbf{KMO}=\frac{\sum_{i=1}^{p}\sum_{j=1}^{p}r_{ij}^2}{\sum_{i=1}^{p}\sum_{j=1}^{p}r_{ij}^2+\sum_{i=1}^{p}\sum_{j=1}^{p}a_{ij}^2}
$$
em que $r_{ij}$ é o coeficiente de correlação simples e $a_{ij}$ é a correlação parcial.

Quanto mais $\sum_{i=1}^{p}\sum_{j=1}^{p}a_{ij}^2$ for próximo de 0, mais o valor de $\mathbf{KMO}$ se aproximará de 1 e mais adequados os dados serão para a Análise fatorial. Valores abaixo de 0,6 são considerados ruins para a Análise fatorial.

```{r estat12, warning=FALSE, message=FALSE}

#Correlaçoes Parciais - Matriz anti-imagem coeficientes de corr
partial.cor <- function (X, ...)
{
  R <- cor(X, ...)
  RI <- solve(R)
  D <- 1/sqrt(diag(RI))
  Rp <- -RI * (D %o% D)
  diag(Rp) <- 0
  rownames(Rp) <- colnames(Rp) <- colnames(X)
  Rp
}
matcorp <- partial.cor(dados)
print(matcorp, digits=2)

#Estatística KMO
KMO(cor(dados))

# Bartlett teste de esfericidade
cortest.bartlett(cor(dados), n = nrow(dados))

bart_spher(dados)
```

<br>
